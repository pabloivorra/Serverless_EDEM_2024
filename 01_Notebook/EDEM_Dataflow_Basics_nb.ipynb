{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script: Dataflow Basics\n",
    "\n",
    "Description: Notebook where we will see the functioning of each transformation discussed during the theory.\n",
    "\n",
    "EDEM. Master Data Analytics<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/beam_pipeline.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apache Beam is a unified programming model for parallel data processing, providing a set of transformations that enable efficient manipulation and processing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline, PCollection & PTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **PCollection**: In Apache Beam, *PCollection* represents an immutable collection of data flowing through a processing pipeline.\n",
    "\n",
    "- **Pipeline**: A *Pipeline* in Apache Beam defines a data processing flow, specifying the sequence of transformations to be applied to the PCollections.\n",
    "\n",
    "- **PTransform**: *PTransform* is an abstraction in Apache Beam that encapsulates a data transformation operation. It defines how an input PCollection is transformed into an output PCollection within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    (p   \n",
    "        | \"Read Text from a File\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "        | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "        | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParDo vs Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ParDo: Applies a function to each element in a data bundle, allowing for more complex and flexible operations than the Map transformation.\n",
    "\n",
    "DoFn: Defines a function that can be used in ParDo transformations to perform more advanced and customized operations on the elements of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "def edem_map(element, num):\n",
    "    return element * num\n",
    "\n",
    "# DoFn\n",
    "class edemDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self, num):\n",
    "        self.num_ = num\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element * self.num_\n",
    "\n",
    "# Pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  data = (\n",
    "      p \n",
    "        | \"Create a PCollection\" >> beam.Create([1,2,3,4,5])\n",
    "        | \"Map\" >> beam.Map(edem_map, num=2)\n",
    "        | \"DoFn\" >> beam.ParDo(edemDoFn(4))\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DoFn Life Cycle\n",
    "\n",
    "The life cycle of a **DoFn** in Apache Beam refers to the phases that an instance of the DoFn class goes through from its initialization to its completion within the context of a *ParDo* transformation. Below, I describe the main stages of the DoFn life cycle:\n",
    "\n",
    "1. **Initialization: Setup**:\n",
    "\n",
    "   - Goal: This phase occurs once for each instance of DoFn before the transformation is executed.\n",
    "   - Process: Initialization tasks, such as configuring resources and preparing data that will be used during execution, are performed here.\n",
    "\n",
    "2. **Processing Elements (Process)**:\n",
    "\n",
    "   - Goal: This phase is executed for each input element in out data bundle.\n",
    "   - Process: The core processing logic is implemented in the *process(self,element)* method of the DoFn class. This method is called for each element and defines how each input is processed.\n",
    "\n",
    "3. **Start bundle or finish buncle**:\n",
    "\n",
    "   - Goal: This phase occurs once before/after all elements in a bundle (a portion of data processed in parallel) are being or have been processed.\n",
    "   - Process: Clean-up tasks and resource finalization used during bundle processing are carried out here.\n",
    "\n",
    "4. **Teardown**: \n",
    "\n",
    "   - Goal: This phase occurs once after all elements in the dataset have been processed.\n",
    "   - Process: Final cleaning and resource release tasks are executed, ensuring that all closing operations are completed successfully.\n",
    "\n",
    "The life cycle of a **DoFn** provides structured control over the execution of processing logic in *ParDo* transformations. **Each instance of DoFn is created, initialized, processes logic for each element, and is closed in a controlled manner**. This approach ensures proper resource management and facilitates the implementation of custom and clean data processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class DoFnLifeCycle(beam.DoFn):\n",
    "\n",
    "  def now(self):\n",
    "    self._now = datetime.now()\n",
    "    return self._now\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"Constructor started at: %s\" % self.now())\n",
    "\n",
    "  def setup(self):\n",
    "    print(\"worker started at: %s\" % self.now())\n",
    "\n",
    "  def start_bundle(self):\n",
    "    print(\"bundle started at: %s\" % self.now())\n",
    "\n",
    "  def process(self, element):\n",
    "    words = element.split()\n",
    "    for word in words:\n",
    "      print(\"Processing element: %s\" % word.upper())\n",
    "      yield word.upper()\n",
    "\n",
    "  def finish_bundle(self):\n",
    "    print(\"bundle finished at: %s\" % self.now())\n",
    "\n",
    "  def teardown(self):\n",
    "    print(\"worker finished at: %s\" % self.now())\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  input_data = (\n",
    "      p \n",
    "        | \"Reading the input file\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"DoFn Life Cycle\" >> beam.ParDo(DoFnLifeCycle())\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GroupByKey\n",
    "\n",
    "It Groups the elements of a data bundle according to a common key, generating a set where the keys are unique, and the values are lists of elements associated with each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CoGroupByKey\n",
    "\n",
    "It merges two PCollections by key, generating pairs of keys and lists of associated elements from both data bundles. Used to perform operations involving data from two different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "\n",
    "    data | \"Print\" >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flatten\n",
    "\n",
    "It combines multiple PCollections into a single collection, flattening the nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partition\n",
    "\n",
    "it splits a PCollection into different partitions based on certain criteria, enabling parallel and distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def partition_fn(country,num_countries):\n",
    "    return countries.index(country['country'])\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        p1,p2,p3 = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        {'country': 'Spain', 'city': 'Valencia'},\n",
    "                        {'country': 'Spain', 'city': 'Barcelona'},\n",
    "                        {'country': 'USA', 'city': 'New York'},\n",
    "                        {'country': 'Switzerland', 'city': 'Zurich'},\n",
    "                        {'country': 'Switzerland', 'city': 'Geneva'}  \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(partition_fn, len(countries))\n",
    "        )\n",
    "\n",
    "        p3 | \"PCollection for Switzerland\" >> beam.Map(print)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine\n",
    "\n",
    "It combines values associated with the same key using a specific combining function, useful for performing key-based aggregate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('User1', 1), ('User2', 5), ('User1', 7)]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gcloud commnads:\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"\"\n",
    "subscription_name = \"\"\n",
    "bq_dataset = \"\"\n",
    "bq_table = \"\"\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PubSub - Dataflow - BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='nombre:STRING', # Required Format: field:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataflow (Google Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        # save_main_session=True\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-west1\"\n",
    "    )) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = \"\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='', # Required Format: field:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window in Apache Beam:**\n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based specific operations.\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** Groups elements into fixed time intervals, dividing the PCollection into time-based windows.\n",
    "\n",
    "- **Sliding Window:** Allows overlapping windows, specified by a size and a stride, making it easy to analyze data in continuous intervals over time.\n",
    "\n",
    "- **Session Window:** Groups data elements that share a contiguous temporal relationship, where continuity is defined by the **inactivity gap between elements**. This dynamic window is formed based on the inactivity time between events, allowing the capture of logical sessions in data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/fixed_window.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Fixed Window\" >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "            | \"Combine\" >> beam.CombineGlobally(sum).without_defaults()\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sliding windows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/sliding_window.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return output['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Sliding Window\" >> beam.WindowInto(beam.window.SlidingWindows(size=60, period=20))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb80d8843f3fcc2b8a9e4270eecba2ac31de31ca61bfe968e7b44b35850adf72"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
